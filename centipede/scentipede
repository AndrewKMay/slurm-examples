#!/bin/bash

# Generic shell script for submitting many thousand short running jobs to a
# slurm cluster. The script goes through 4 phases of a typical HPC pipeline:
# 1. Data preparation, 2. parallel execution, 3. error correction, 4. merge
# of data. It assumes that you want to launch another script (e.g. using R
# or Python) which is set in the SCRIPT variable below. In this case it
# assumes that SCRIPT takes several command line args each of which tell
# SCRIPT to execute one of the 4 different phases. Arguments for example.R:
# Phase 0: ./example.R listsize  (get array size - not run via slurm)
# Phase 1: ./example.R prepare   (initial data preparation)
# Phase 2: ./example.R run xx    (run listsize # of jobs)
# Phase 3: ./example.R run xx    (only the failed jobs having no outfile) 
# Phase 4: ./example.R merge     (merge the output files) 
# This script (submit slurm) can take 2 arguments, SCRIPT and JOBNAME

##################### Change these constants ##############################

JOBNAME='myJob'            # change for every analysis you run (2nd arg)
MAILDOM='@fredhutch.org'   # your email domain (for receiving error messages)
MAXARRAYSIZE=1000          # set to 0 if you are not using slurm job arrays
MYSCRATCH="./scratch/${JOBNAME}"  # location of your persistent scratch dir
PARTITION='restart'        # the queue on your cluster that allows short jobs
RESULTDIR="./result/${JOBNAME}"  # This is a folder in permanent storage
SCRIPT='./example.R'       # your code as (R or Python) script (1st arg)
STEPSIZE=2                 # number of consequtive loops in SCRIPT to run in
                           # the same job / node (increase for short jobs)
                           
############## typically you don't have to change anything below here #######

username=$(id -nu)
listsize=$(${SCRIPT} listsize)   # call SCRIPT to get the number of loops

[[ -z $1 ]] || SCRIPT=$1   # get SCRIPT and JOBNAME from command line
oldjobname=$JOBNAME
[[ -z $2 ]] || JOBNAME=$2
MYSCRATCH=${MYSCRATCH//"$oldjobname"/"$JOBNAME"}
RESULTDIR=${RESULTDIR//"$oldjobname"/"$JOBNAME"}
[[ -d ${MYSCRATCH}/output ]] && echo "${MYSCRATCH}/output already exists !!"
hash ${SCRIPT} 2>/dev/null || { echo "${SCRIPT} not found, exiting !!" ; exit ; }
mkdir -p "${MYSCRATCH}/output"
mkdir -p "${MYSCRATCH}/run"
mkdir -p "${RESULTDIR}"
export MYSCRATCH           # set static vars globally so cluster jobs launched
export RESULTDIR           # from this script can read them as an environment
export STEPSIZE            # variables

echo "using $SCRIPT with args $@..." 

# run the part below if $RETRYFAILED environment var is not set or ""
if [[ -z $RETRYFAILED ]]; then 

    # Preparing input data serially, pass 'prepare' parameter to SCRIPT,
    echo "submitting ${SCRIPT} prepare in ${MYSCRATCH}..."
    sbatch --dependency=singleton --job-name=${JOBNAME} --partition=${PARTITION} \
           --mail-type=FAIL --mail-user="${username}{MAILDOM}" \
           --output="${MYSCRATCH}/output/${JOBNAME}.prepare.%J" --requeue --time=0-3 \
           --wrap="${SCRIPT} prepare $3 $4 $5 $6 $7 $8 $9"

    # get a colon separated list of job ids which must end before the next phase can start
    waitforjobs=$(squeue --format="%A" -h -u ${username} -n ${JOBNAME} --sort=i | tr "\n" ":")
    waitforjobs=${waitforjobs%?} #remove the last character (:) from string

    # Running multiple jobs (array or standard), pass 'run' and 'id' to each SCRIPT (1h each)
    if [[ $MAXARRAYSIZE -gt 1 ]]; then
        # running array jobs
        numjobs=$((${listsize}/${MAXARRAYSIZE}+1))
        arrid=0
        arrupper=$MAXARRAYSIZE
        for i in $(seq 1 $numjobs); do
            if [[ $i -eq $numjobs ]]; then
                arrupper=$((${listsize}-${arrid}))
            fi
            echo "submitting ${SCRIPT} run in ${MYSCRATCH} with ${STEPSIZE} ${arrid} dependent on ${waitforjobs}..."
            sbatch --array=1-${arrupper}:${STEPSIZE} --dependency=afterok:${waitforjobs} --job-name=${JOBNAME} \
                   --partition=${PARTITION} --mail-type=FAIL --mail-user="${username}${MAILDOM}" \
                   --output="${MYSCRATCH}/output/${JOBNAME}.run.${arrid}_%a_%A.%J" --requeue --time=0-1 \
                   --wrap="${SCRIPT} run ${arrid} $3 $4 $5 $6 $7 $8 $9"
            arrid=$((${arrid}+${MAXARRAYSIZE}))
        done
    else
        # running standard jobs, limit the number of pending jobs to 300
        numjobs=$((${listsize}/${STEPSIZE}))
        c=0
        for i in $(seq 1 $numjobs); do
            echo "submitting ${SCRIPT} run in ${MYSCRATCH} with ${STEPSIZE} ${id} dependent on ${waitforjob}..."
            id=$((${i}*${STEPSIZE}-${STEPSIZE}+1))
            sbatch --dependency=afterok:${waitforjobs} --job-name=${JOBNAME} \
                   --mail-type=FAIL --mail-user="${username}${MAILDOM}" --partition=${PARTITION} \
                   --output="${MYSCRATCH}/output/${JOBNAME}.run.${i}.%J" --requeue --time=0-1 \
                   --wrap="${SCRIPT} run ${id} $3 $4 $5 $6 $7 $8 $9"
            ((c+=1))
            if [[ $c -gt 100 ]]; then
                # check for pending jobs 
                echo -e "wait for pending jobs to finish..."
                c=0 
                wait=1
                while [[ $wait -eq 1 ]]; do
                    pending=$(squeue --state PD --format="%A" -h -u ${username} -n ${JOBNAME} | wc -l | bc)
                    if [[ $pending -lt 300 ]]; then
                        wait=0
                    fi
                    sleep 1
                done
            fi
        done
    fi
    thisscript=$0
    if ! [[ "$0" = /* ]]; then
        thisscript=$(pwd)/$0
    fi
    # submit control job that waits for parallel jobs to finish and then re-submits failed jobs (4h)
    echo "submitting control job with args '$@' that waits and resubmits failed jobs..."
    sbatch --dependency=singleton --job-name=${JOBNAME} \
               --mail-type=FAIL --mail-user="${username}${MAILDOM}" --partition=${PARTITION} \
               --output="${MYSCRATCH}/output/${JOBNAME}.correct.%J" --requeue --time=0-4 \
               --wrap="RETRYFAILED=$numjobs ${thisscript} $1 $2 $3 $4 $5 $6 $7 $8 $9"

else
# run the part below if $RETRYFAILED environment var IS SET by previous control job
# the control job is needed because we cannot submit retry jobs right at the 
# beginning as we do not yet know which jobs will fail.

    # re-run all failed jobs where the appropriate output file is missing  (2h)
    # --requeue works only for preempted jobs 
    for i in $(seq 1 $RETRYFAILED); do
        id=$((${i}*${STEPSIZE}-${STEPSIZE}+1))
        if ! [[ -f "${MYSCRATCH}/run/${i}-run.dat" ]]; then
            echo "re-submitting ${SCRIPT} run ${id}"
            sbatch --dependency=singleton --job-name=${JOBNAME} \
                   --mail-type=FAIL --mail-user="${username}${MAILDOM}" --partition=${PARTITION} \
                   --output="${MYSCRATCH}/output/${JOBNAME}.run2.${i}.%J" --requeue --time=0-2 \
                   --wrap="${SCRIPT} run ${id} $3 $4 $5 $6 $7 $8 $9"
        fi
    done

    # Merge results serially, pass 2 parameters to SCRIPT, 2nd is optional (5h)
    echo "submitting ${SCRIPT} merge ${listsize}..."
    sbatch --dependency=singleton --job-name=${JOBNAME} --partition=${PARTITION} \
           --mail-type=END,FAIL --mail-user="${username}${MAILDOM}" \
           --output="${MYSCRATCH}/output/${JOBNAME}.merge.%J" --requeue --time=0-5 \
           --wrap="${SCRIPT} merge ${listsize} $3 $4 $5 $6 $7 $8 $9"

fi

echo -e "monitor output with this command:"
echo -e "tail -f ${MYSCRATCH}/output/${JOBNAME}.*"
